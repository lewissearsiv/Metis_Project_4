{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import sklearn\n",
    "from nltk.tokenize import MWETokenizer \n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24110"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################################\n",
    "#Data graciously taken from https://www.thetrumparchive.com/#\n",
    "#############################################################\n",
    "df_tweets = pd.read_csv('trump_tweets_raw.csv')\n",
    "pd.to_datetime(df_tweets['date'])\n",
    "df_tweets.sort_values('date',ascending = False, inplace = True,ignore_index = True)\n",
    "df_tweets=df_tweets.rename(columns = {'date':'DateTime'})\n",
    "df_tweets['DateTime'] = [datetime.datetime.strptime(d, \"%Y-%m-%d %H:%M:%S\") for d in df_tweets[\"DateTime\"]]\n",
    "df_tweets['Date'] = [datetime.datetime.date(d) for d in df_tweets['DateTime']] \n",
    "df_tweets['Time'] = [datetime.datetime.time(d) for d in df_tweets['DateTime']] \n",
    "#The Day trump became president\n",
    "mask = (df_tweets['Date'] >= datetime.date(2017, 1, 20))\n",
    "df_trump = df_tweets[mask]\n",
    "len(df_trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word_tokenize(text):\n",
    "    '''In order: remove punctuation, make lowercase, remove numbers,\n",
    "    tokenize by words and multi words, remove stop-words'''\n",
    "    \n",
    "        #Set your own stopwords here:\n",
    "    stop_words = stopwords.words('english')\n",
    "    additional_stop_words = ['rt']\n",
    "    stop_words.extend(additional_stop_words)\n",
    "    \n",
    "        #Put your own group of words in the MWETokenizer\n",
    "    mwe_tokenizer = MWETokenizer([('make','america','great','again'),('america','first')])\n",
    "    \n",
    "        #Begin Cleaning\n",
    "    text_no_punctuation = re.sub('[%s]' % re.escape(string.punctuation),'',text)\n",
    "    text_lower = text_no_punctuation.lower()\n",
    "    text_no_num = re.sub('\\w*\\d\\w*','',text_lower)\n",
    "    tokenize_text = mwe_tokenizer.tokenize(word_tokenize(text_no_num))\n",
    "    no_stop_text = [word for word in tokenize_text if not word in stop_words]\n",
    "    return no_stop_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>isDeleted</th>\n",
       "      <th>device</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1320096006038380544</td>\n",
       "      <td>Just landed in Ohio. See you in a little while!</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>37662</td>\n",
       "      <td>4948</td>\n",
       "      <td>2020-10-24 20:13:08</td>\n",
       "      <td>2020-10-24</td>\n",
       "      <td>20:13:08</td>\n",
       "      <td>[landed, ohio, see, little]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1320095628546899968</td>\n",
       "      <td>Nobody is showing up for Obama’s hate laced sp...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>57061</td>\n",
       "      <td>10961</td>\n",
       "      <td>2020-10-24 20:11:38</td>\n",
       "      <td>2020-10-24</td>\n",
       "      <td>20:11:38</td>\n",
       "      <td>[nobody, showing, obama, ’, hate, laced, speec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1320076630065373184</td>\n",
       "      <td>AMERICA FIRST!</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>89661</td>\n",
       "      <td>14937</td>\n",
       "      <td>2020-10-24 18:56:09</td>\n",
       "      <td>2020-10-24</td>\n",
       "      <td>18:56:09</td>\n",
       "      <td>[america_first]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1320076502839459842</td>\n",
       "      <td>MAKE AMERICA GREAT AGAIN!</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>152417</td>\n",
       "      <td>24873</td>\n",
       "      <td>2020-10-24 18:55:38</td>\n",
       "      <td>2020-10-24</td>\n",
       "      <td>18:55:38</td>\n",
       "      <td>[make_america_great_again]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1320076289034850306</td>\n",
       "      <td>Joe Biden = Biggest Tax Increase In History an...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>57012</td>\n",
       "      <td>11744</td>\n",
       "      <td>2020-10-24 18:54:47</td>\n",
       "      <td>2020-10-24</td>\n",
       "      <td>18:54:47</td>\n",
       "      <td>[joe, biden, biggest, tax, increase, history, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1320096006038380544    Just landed in Ohio. See you in a little while!   \n",
       "1  1320095628546899968  Nobody is showing up for Obama’s hate laced sp...   \n",
       "2  1320076630065373184                                     AMERICA FIRST!   \n",
       "3  1320076502839459842                          MAKE AMERICA GREAT AGAIN!   \n",
       "4  1320076289034850306  Joe Biden = Biggest Tax Increase In History an...   \n",
       "\n",
       "  isRetweet isDeleted              device  favorites  retweets  \\\n",
       "0         f         f  Twitter for iPhone      37662      4948   \n",
       "1         f         f  Twitter for iPhone      57061     10961   \n",
       "2         f         f  Twitter for iPhone      89661     14937   \n",
       "3         f         f  Twitter for iPhone     152417     24873   \n",
       "4         f         f  Twitter for iPhone      57012     11744   \n",
       "\n",
       "             DateTime        Date      Time  \\\n",
       "0 2020-10-24 20:13:08  2020-10-24  20:13:08   \n",
       "1 2020-10-24 20:11:38  2020-10-24  20:11:38   \n",
       "2 2020-10-24 18:56:09  2020-10-24  18:56:09   \n",
       "3 2020-10-24 18:55:38  2020-10-24  18:55:38   \n",
       "4 2020-10-24 18:54:47  2020-10-24  18:54:47   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0                        [landed, ohio, see, little]  \n",
       "1  [nobody, showing, obama, ’, hate, laced, speec...  \n",
       "2                                    [america_first]  \n",
       "3                         [make_america_great_again]  \n",
       "4  [joe, biden, biggest, tax, increase, history, ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trump['tokenized_text'] = df_trump['text'].apply(clean_word_tokenize)\n",
    "df_trump.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a CountVectorizer DataFrame\n",
    "cv = CountVectorizer(analyzer=lambda x:x)\n",
    "vectorized_words = cv.fit_transform(df_trump['tokenized_text']).toarray()\n",
    "col_names = cv.get_feature_names()\n",
    "df_vectorized = pd.DataFrame(vectorized_words, columns = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aap</th>\n",
       "      <th>aapsonline</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abaco</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abaondon…</th>\n",
       "      <th>abbas</th>\n",
       "      <th>...</th>\n",
       "      <th>🤦🏼‍♂️🤦🏼‍♂️🤦🏼‍♂️🤦🏼‍♂️🤦🏼‍♂️🤦🏼‍♂️👇🏻👇🏻</th>\n",
       "      <th>🤨</th>\n",
       "      <th>🤳</th>\n",
       "      <th>🥳</th>\n",
       "      <th>🥳🎂</th>\n",
       "      <th>🥳🙏🙌</th>\n",
       "      <th>🦃</th>\n",
       "      <th>🦅✈️🇺🇸</th>\n",
       "      <th>🧐🧐🧐🧐</th>\n",
       "      <th>🧵thread🧵</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa  aap  aapsonline  aaron  ab  abaco  abandon  abandoned  abaondon…  \\\n",
       "0    0    0           0      0   0      0        0          0          0   \n",
       "1    0    0           0      0   0      0        0          0          0   \n",
       "2    0    0           0      0   0      0        0          0          0   \n",
       "3    0    0           0      0   0      0        0          0          0   \n",
       "4    0    0           0      0   0      0        0          0          0   \n",
       "\n",
       "   abbas  ...  🤦🏼‍♂️🤦🏼‍♂️🤦🏼‍♂️🤦🏼‍♂️🤦🏼‍♂️🤦🏼‍♂️👇🏻👇🏻  🤨  🤳  🥳  🥳🎂  🥳🙏🙌  🦃  🦅✈️🇺🇸  \\\n",
       "0      0  ...                                   0  0  0  0   0    0  0      0   \n",
       "1      0  ...                                   0  0  0  0   0    0  0      0   \n",
       "2      0  ...                                   0  0  0  0   0    0  0      0   \n",
       "3      0  ...                                   0  0  0  0   0    0  0      0   \n",
       "4      0  ...                                   0  0  0  0   0    0  0      0   \n",
       "\n",
       "   🧐🧐🧐🧐  🧵thread🧵  \n",
       "0     0         0  \n",
       "1     0         0  \n",
       "2     0         0  \n",
       "3     0         0  \n",
       "4     0         0  \n",
       "\n",
       "[5 rows x 25512 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aaa           1\n",
       "aap           9\n",
       "aapsonline    1\n",
       "aaron         1\n",
       "ab            2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_sums = df_vectorized.sum(axis = 0)\n",
    "col_sums.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "’                  5911\n",
       "great              3447\n",
       "amp                3095\n",
       "“                  3095\n",
       "”                  2811\n",
       "president          2738\n",
       "realdonaldtrump    2358\n",
       "people             2003\n",
       "trump              1996\n",
       "democrats          1665\n",
       "thank              1530\n",
       "us                 1491\n",
       "news               1459\n",
       "country            1284\n",
       "new                1265\n",
       "big                1200\n",
       "fake               1182\n",
       "today              1072\n",
       "get                1071\n",
       "would              1048\n",
       "many               1029\n",
       "never              1016\n",
       "american           1013\n",
       "time                911\n",
       "one                 873\n",
       "want                870\n",
       "years               866\n",
       "media               859\n",
       "biden               830\n",
       "america             828\n",
       "house               825\n",
       "border              820\n",
       "whitehouse          818\n",
       "good                794\n",
       "like                792\n",
       "vote                780\n",
       "much                778\n",
       "even                777\n",
       "states              769\n",
       "back                749\n",
       "going               741\n",
       "china               716\n",
       "united              710\n",
       "done                708\n",
       "joe                 702\n",
       "state               696\n",
       "job                 691\n",
       "must                667\n",
       "nothing             648\n",
       "impeachment         645\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_sums.sort_values(ascending = False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lews', 'party', 'awesome']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
