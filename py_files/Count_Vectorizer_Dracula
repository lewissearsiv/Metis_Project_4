'''This is a simple and adjustable cleaning function for NLP which takes
a string and returns clean tokens for analysis. Used for sentiment analysis
of trumps tweets, but easy plug in play for other general projects.
'''
import pandas as pd
pd.set_option('display.max_rows', 500)
import numpy as np
import datetime
import re
import string
import nltk
import sklearn
from nltk.tokenize import MWETokenizer 
from nltk import word_tokenize,sent_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
    #py file in directory
from py_files.cleaner import mr_clean_and_tokenize
pd.set_option('display.max_rows', 500)
import warnings
warnings.filterwarnings('ignore')


#Make a CountVectorizer DataFrame
cv = CountVectorizer(analyzer=lambda x:x)
vectorized_words = cv.fit_transform(df_trump['tokenized_text']).toarray()
col_names = cv.get_feature_names()
df_vectorized = pd.DataFrame(vectorized_words, columns = col_names)

#Most used words
col_sums = df_vectorized.sum(axis = 0)
important_words = [] 
for i in range(len(col_sums)):
    #Give a threshhold minimum value
    if col_sums[i] > 2:
        important_words.append(col_sums.index[i])